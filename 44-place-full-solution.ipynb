{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8719e5d2",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2022-02-02T09:43:10.039111Z",
     "iopub.status.busy": "2022-02-02T09:43:10.038477Z",
     "iopub.status.idle": "2022-02-02T09:43:13.232323Z",
     "shell.execute_reply": "2022-02-02T09:43:13.231651Z",
     "shell.execute_reply.started": "2022-01-30T14:48:53.143544Z"
    },
    "papermill": {
     "duration": 3.212844,
     "end_time": "2022-02-02T09:43:13.232477",
     "exception": false,
     "start_time": "2022-02-02T09:43:10.019633",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type='text/css'>\n",
       ".datatable table.frame { margin-bottom: 0; }\n",
       ".datatable table.frame thead { border-bottom: none; }\n",
       ".datatable table.frame tr.coltypes td {  color: #FFFFFF;  line-height: 6px;  padding: 0 0.5em;}\n",
       ".datatable .bool    { background: #DDDD99; }\n",
       ".datatable .object  { background: #565656; }\n",
       ".datatable .int     { background: #5D9E5D; }\n",
       ".datatable .float   { background: #4040CC; }\n",
       ".datatable .str     { background: #CC4040; }\n",
       ".datatable .time    { background: #40CC40; }\n",
       ".datatable .row_index {  background: var(--jp-border-color3);  border-right: 1px solid var(--jp-border-color0);  color: var(--jp-ui-font-color3);  font-size: 9px;}\n",
       ".datatable .frame tbody td { text-align: left; }\n",
       ".datatable .frame tr.coltypes .row_index {  background: var(--jp-border-color0);}\n",
       ".datatable th:nth-child(2) { padding-left: 12px; }\n",
       ".datatable .hellipsis {  color: var(--jp-cell-editor-border-color);}\n",
       ".datatable .vellipsis {  background: var(--jp-layout-color0);  color: var(--jp-cell-editor-border-color);}\n",
       ".datatable .na {  color: var(--jp-cell-editor-border-color);  font-size: 80%;}\n",
       ".datatable .sp {  opacity: 0.25;}\n",
       ".datatable .footer { font-size: 9px; }\n",
       ".datatable .frame_dimensions {  background: var(--jp-border-color3);  border-top: 1px solid var(--jp-border-color0);  color: var(--jp-ui-font-color3);  display: inline-block;  opacity: 0.6;  padding: 1px 10px 1px 5px;}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# basic\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import os\n",
    "import random\n",
    "\n",
    "# cross validation\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "# models\n",
    "from sklearn.tree import ExtraTreeClassifier  \n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import OneClassSVM\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.neighbors import RadiusNeighborsClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.linear_model import RidgeClassifier\n",
    "from sklearn.linear_model import PassiveAggressiveClassifier    \n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier as rf\n",
    "from lightgbm import LGBMClassifier as lgbm\n",
    "from catboost import CatBoostClassifier as catboost\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from xgboost import XGBClassifier as xgb\n",
    "\n",
    "\n",
    "# evaluating\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "#optimization\n",
    "import optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9f8d0b7a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-02T09:43:13.259258Z",
     "iopub.status.busy": "2022-02-02T09:43:13.258564Z",
     "iopub.status.idle": "2022-02-02T09:43:13.596158Z",
     "shell.execute_reply": "2022-02-02T09:43:13.596610Z",
     "shell.execute_reply.started": "2022-01-30T14:48:56.276716Z"
    },
    "papermill": {
     "duration": 0.352482,
     "end_time": "2022-02-02T09:43:13.596782",
     "exception": false,
     "start_time": "2022-02-02T09:43:13.244300",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "ss = pd.read_csv(\"/kaggle/input/song-popularity-prediction/sample_submission.csv\")\n",
    "train = pd.read_csv(\"/kaggle/input/song-popularity-prediction/train.csv\")\n",
    "test = pd.read_csv(\"/kaggle/input/song-popularity-prediction/test.csv\")\n",
    "\n",
    "TRAIN_SIZE = 0.5\n",
    "VALID_SIZE = 0.5\n",
    "\n",
    "TARGET=\"song_popularity\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9546ac7f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-02T09:43:13.617908Z",
     "iopub.status.busy": "2022-02-02T09:43:13.617280Z",
     "iopub.status.idle": "2022-02-02T09:43:13.627538Z",
     "shell.execute_reply": "2022-02-02T09:43:13.627993Z",
     "shell.execute_reply.started": "2022-01-30T14:48:56.618016Z"
    },
    "papermill": {
     "duration": 0.022592,
     "end_time": "2022-02-02T09:43:13.628189",
     "exception": false,
     "start_time": "2022-02-02T09:43:13.605597",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def FE(df):\n",
    "    \n",
    "    def logme(ser: pd.Series):\n",
    "        abs_ = ser.abs()\n",
    "        abs_ = np.where(abs_ == 0, 0.001, abs_)\n",
    "        return np.log(abs_)\n",
    "\n",
    "    def update_trims(df):\n",
    "            final_lower_trims = {'song_duration_ms': 1,\n",
    "                                 'acousticness': 0,\n",
    "                                 'danceability': 1,\n",
    "                                 'energy': 0,\n",
    "                                 'instrumentalness': 0,\n",
    "                                 'liveness': 0,\n",
    "                                 'loudness': 0,\n",
    "                                 'speechiness': 1,\n",
    "                                 'tempo': 0,\n",
    "                                 'time_signature': 1,\n",
    "                                 'audio_valence': 0}\n",
    "\n",
    "\n",
    "            final_upper_trims = {'song_duration_ms': 1,\n",
    "                                 'acousticness': 1,\n",
    "                                 'danceability': 1,\n",
    "                                 'energy': 1,\n",
    "                                 'instrumentalness': 1,\n",
    "                                 'liveness': 1,\n",
    "                                 'loudness': 1,\n",
    "                                 'speechiness': 1,\n",
    "                                 'tempo': 1,\n",
    "                                 'time_signature': 1,\n",
    "                                 'audio_valence': 1}\n",
    "            \n",
    "            \n",
    "            for col in df.columns:\n",
    "                if col in final_upper_trims:\n",
    "                    max_ = df[col].max()\n",
    "                    min_ =  df[col].min()\n",
    "                    p = (max_ - min_) / 100\n",
    "                    new_max = max_ - (p * final_upper_trims[col])\n",
    "                    new_min = min_ + (p * final_lower_trims[col])\n",
    "                    df[col] = np.clip(df[col], new_min, new_max)\n",
    "            return df\n",
    "\n",
    "    \n",
    "    df[\"acousticness\"] =  logme(df[\"acousticness\"])\n",
    "    df[\"danceability\"] =  logme(df[\"danceability\"])\n",
    "    df[\"instrumentalness\"] =  logme(df[\"instrumentalness\"])\n",
    "    df[\"liveness\"] =  logme(df[\"liveness\"])\n",
    "    df[\"speechiness\"] =  logme(df[\"speechiness\"])    \n",
    "    \n",
    "    df = update_trims(df)\n",
    "    \n",
    "    df = df.fillna(df.median())\n",
    "    \n",
    "    return df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1458304b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-02T09:43:13.657773Z",
     "iopub.status.busy": "2022-02-02T09:43:13.657083Z",
     "iopub.status.idle": "2022-02-02T09:43:13.659621Z",
     "shell.execute_reply": "2022-02-02T09:43:13.660140Z",
     "shell.execute_reply.started": "2022-01-30T14:48:56.631374Z"
    },
    "papermill": {
     "duration": 0.023289,
     "end_time": "2022-02-02T09:43:13.660330",
     "exception": false,
     "start_time": "2022-02-02T09:43:13.637041",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_N_models(X,y):\n",
    "    models = {} \n",
    "    \n",
    "    lgbm_parms   = {'boosting_type': 'dart', 'num_leaves': 16, 'n_estimators': 438, 'max_depth': 33, 'min_samples_leaf': 6, 'learning_rate': 0.05666201057300166, 'subsample': 0.6846131162945679, 'reg_alpha': 0.8841562054396195, 'reg_lambda': 0.43480247488835144}\n",
    "    lgbm_model = lgbm(**lgbm_parms)\n",
    "    lgbm_model.fit(X,y) \n",
    "    models[\"lgbm\"]= lgbm_model\n",
    "    \n",
    "    rf_parms = {'random_state': 2247,'n_estimators': 182, 'max_depth': 9, 'min_samples_split': 36, 'min_samples_leaf': 57, 'max_features': 13}\n",
    "    rf_model = rf(**rf_parms) \n",
    "    rf_model.fit(X,y) \n",
    "    models[\"rf\"]= rf_model\n",
    "    \n",
    "    GB_parms ={'n_estimators': 333, 'max_depth': 3, 'min_samples_split': 58, 'min_samples_leaf': 16, 'max_features': 9, 'learning_rate': 0.05690001817211911, 'subsample': 0.8865189659366153, 'criterion': 'mse'}\n",
    "    GB_mosel = GradientBoostingClassifier(**GB_parms)\n",
    "    GB_mosel.fit(X,y)\n",
    "    models[\"GB\"]= GB_mosel\n",
    "\n",
    "    cat_parms = {'objective': 'Logloss', 'colsample_bylevel': 0.08751674271524071, 'depth': 3, 'boosting_type': 'Plain', 'bootstrap_type': 'MVS', 'iterations': 178, 'learning_rate': 0.15825880083719676, 'random_strength': 92, 'od_type': 'IncToDec', 'verbose': 0}\n",
    "    cat_model = catboost(**cat_parms)\n",
    "    cat_model.fit(X,y)\n",
    "    models[\"cat\"]= cat_model \n",
    "    \n",
    "    xgb_parms = {'n_estimators': 84, 'booster': 'gbtree', 'max_depth': 3, 'min_samples_split': 68, 'min_samples_leaf': 32, 'max_features': 3, 'learning_rate': 0.1011207776914081, 'reg_alpha': 0.2769253824871142, 'reg_lambda': 0.38851574155058827, 'gamma': 0.914525582012808, 'subsample': 0.8187435357656347}\n",
    "    xgb_model = xgb(**xgb_parms)\n",
    "    xgb_model.fit(X,y)\n",
    "    models[\"xgb\"] = xgb_model\n",
    "    \n",
    "\n",
    "    ETC_parms = {'n_estimators': 472, 'max_depth': 12, 'min_samples_split': 107, 'min_samples_leaf': 9, 'max_features': 13}\n",
    "    ETC_model = ExtraTreesClassifier(**ETC_parms)\n",
    "    ETC_model.fit(X,y)\n",
    "    models[\"ETC\"]= ETC_model \n",
    "    \n",
    "    \n",
    "    return models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2a04441d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-02T09:43:13.681054Z",
     "iopub.status.busy": "2022-02-02T09:43:13.680461Z",
     "iopub.status.idle": "2022-02-02T09:43:13.685949Z",
     "shell.execute_reply": "2022-02-02T09:43:13.686450Z",
     "shell.execute_reply.started": "2022-01-30T14:48:56.646452Z"
    },
    "papermill": {
     "duration": 0.017387,
     "end_time": "2022-02-02T09:43:13.686611",
     "exception": false,
     "start_time": "2022-02-02T09:43:13.669224",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def predict_N_models(X,models, ens_method=\"avg\"):\n",
    "    preds_df = pd.DataFrame()\n",
    "    for model in models:\n",
    "        if model==\"SG\":\n",
    "            preds_df[model] = models[model].predict(X)\n",
    "\n",
    "        else:\n",
    "            pos_ind = list(models[model].classes_).index(1)\n",
    "            preds = models[model].predict_proba(X)\n",
    "            preds_df[model] = [preds[row][pos_ind] for row in range(preds.shape[0])]\n",
    "        \n",
    "    if ens_method == \"avg\":\n",
    "        return preds_df.mean(axis = 1)\n",
    "    \n",
    "    elif ens_method == \"ens\":\n",
    "        return preds_df\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "23c63b7d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-02T09:43:13.707805Z",
     "iopub.status.busy": "2022-02-02T09:43:13.707129Z",
     "iopub.status.idle": "2022-02-02T09:43:13.711519Z",
     "shell.execute_reply": "2022-02-02T09:43:13.711963Z",
     "shell.execute_reply.started": "2022-01-30T14:48:56.661479Z"
    },
    "papermill": {
     "duration": 0.016411,
     "end_time": "2022-02-02T09:43:13.712144",
     "exception": false,
     "start_time": "2022-02-02T09:43:13.695733",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_ens(models_preds, y):\n",
    "    parms = {'verbose': 0, 'objective': 'CrossEntropy', 'colsample_bylevel': 0.058798132946050255, 'depth': 4, 'boosting_type': 'Ordered', 'bootstrap_type': 'Bayesian', 'iterations': 78, 'learning_rate': 0.01713074463652201, 'random_strength': 52, 'od_type': 'Iter', 'bagging_temperature': 4.812865115761063}\n",
    "    catboost_model = catboost(**parms)\n",
    "    catboost_model.fit(models_preds,y)\n",
    "\n",
    "    return catboost_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eed89355",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-02T09:43:13.735588Z",
     "iopub.status.busy": "2022-02-02T09:43:13.734985Z",
     "iopub.status.idle": "2022-02-02T09:43:13.738887Z",
     "shell.execute_reply": "2022-02-02T09:43:13.738342Z",
     "shell.execute_reply.started": "2022-01-30T14:48:56.676445Z"
    },
    "papermill": {
     "duration": 0.017735,
     "end_time": "2022-02-02T09:43:13.739033",
     "exception": false,
     "start_time": "2022-02-02T09:43:13.721298",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def predict_ens(preds, ens):\n",
    "    pos_ind = list(ens.classes_).index(1)\n",
    "    preds = ens.predict_proba(preds)\n",
    "    return [preds[row][pos_ind] for row in range(preds.shape[0])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1c8ca1cb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-02T09:43:13.759702Z",
     "iopub.status.busy": "2022-02-02T09:43:13.758805Z",
     "iopub.status.idle": "2022-02-02T09:49:16.261698Z",
     "shell.execute_reply": "2022-02-02T09:49:16.262410Z"
    },
    "papermill": {
     "duration": 362.515089,
     "end_time": "2022-02-02T09:49:16.262601",
     "exception": false,
     "start_time": "2022-02-02T09:43:13.747512",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] min_data_in_leaf is set with min_samples_leaf=6, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[09:44:10] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"max_features\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[09:44:10] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[LightGBM] [Warning] min_data_in_leaf is set with min_samples_leaf=6, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[09:45:21] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"max_features\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[09:45:21] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[LightGBM] [Warning] min_data_in_leaf is set with min_samples_leaf=6, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[09:46:34] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"max_features\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[09:46:34] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[LightGBM] [Warning] min_data_in_leaf is set with min_samples_leaf=6, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[09:47:48] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"max_features\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[09:47:48] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[LightGBM] [Warning] min_data_in_leaf is set with min_samples_leaf=6, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[09:49:00] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"max_features\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[09:49:00] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    }
   ],
   "source": [
    "FOLDS = 5\n",
    "seeds = [947, 27, 1, 2022, 20]\n",
    "\n",
    "test = FE(test)\n",
    "folds_preds = []\n",
    "\n",
    "for fold in range(FOLDS):\n",
    "    random.seed(seeds[fold])\n",
    "    train_index = random.sample(list(train.index), int(train.shape[0] * TRAIN_SIZE))\n",
    "    left_index = [ind for ind in list(train.index) if ind not in train_index]\n",
    "    valid_index = random.sample(left_index, int(train.shape[0] * VALID_SIZE))\n",
    "\n",
    "    X_train, X_valid = train.iloc[train_index].reset_index().drop('index', axis=1), train.iloc[valid_index].reset_index().drop('index', axis=1)\n",
    "    y_train, y_valid = train.iloc[train_index].reset_index()['song_popularity'], train.iloc[valid_index].reset_index()['song_popularity']\n",
    "\n",
    "    X_train = X_train.drop(TARGET,axis=1)\n",
    "    X_valid = X_valid.drop(TARGET,axis=1)\n",
    "\n",
    "    X_train = FE(X_train)\n",
    "    X_valid = FE(X_valid)\n",
    "    \n",
    "    models = train_N_models(X_train.drop(\"id\", axis = 1), y_train)\n",
    "    models_preds = predict_N_models(X_valid.drop(\"id\", axis = 1), models, \"ens\")\n",
    "    ensemble_model = train_ens(models_preds, y_valid)\n",
    "\n",
    "    N_preds = predict_N_models(test.drop(\"id\", axis = 1), models, \"ens\")\n",
    "    folds_preds.append(predict_ens(N_preds, ensemble_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "55ce2c93",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-02T09:49:16.292877Z",
     "iopub.status.busy": "2022-02-02T09:49:16.292281Z",
     "iopub.status.idle": "2022-02-02T09:49:16.301163Z",
     "shell.execute_reply": "2022-02-02T09:49:16.301614Z"
    },
    "papermill": {
     "duration": 0.026148,
     "end_time": "2022-02-02T09:49:16.301790",
     "exception": false,
     "start_time": "2022-02-02T09:49:16.275642",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "final_preds = np.array(folds_preds).mean(axis =0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a31b9612",
   "metadata": {
    "papermill": {
     "duration": 0.012246,
     "end_time": "2022-02-02T09:49:16.326547",
     "exception": false,
     "start_time": "2022-02-02T09:49:16.314301",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "79ed3cf9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-02T09:49:16.354703Z",
     "iopub.status.busy": "2022-02-02T09:49:16.354144Z",
     "iopub.status.idle": "2022-02-02T09:49:16.386425Z",
     "shell.execute_reply": "2022-02-02T09:49:16.386945Z"
    },
    "papermill": {
     "duration": 0.048099,
     "end_time": "2022-02-02T09:49:16.387167",
     "exception": false,
     "start_time": "2022-02-02T09:49:16.339068",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "ss[\"song_popularity\"] = final_preds\n",
    "ss.to_csv(\"submission.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "479280e3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-02T09:49:16.415298Z",
     "iopub.status.busy": "2022-02-02T09:49:16.414726Z",
     "iopub.status.idle": "2022-02-02T09:49:16.430346Z",
     "shell.execute_reply": "2022-02-02T09:49:16.429855Z"
    },
    "papermill": {
     "duration": 0.030629,
     "end_time": "2022-02-02T09:49:16.430482",
     "exception": false,
     "start_time": "2022-02-02T09:49:16.399853",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>song_popularity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.404362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.417344</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0.385500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0.393675</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0.400625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9995</th>\n",
       "      <td>9995</td>\n",
       "      <td>0.396492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9996</th>\n",
       "      <td>9996</td>\n",
       "      <td>0.386148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9997</th>\n",
       "      <td>9997</td>\n",
       "      <td>0.401665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9998</th>\n",
       "      <td>9998</td>\n",
       "      <td>0.414575</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999</th>\n",
       "      <td>9999</td>\n",
       "      <td>0.381214</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        id  song_popularity\n",
       "0        0         0.404362\n",
       "1        1         0.417344\n",
       "2        2         0.385500\n",
       "3        3         0.393675\n",
       "4        4         0.400625\n",
       "...    ...              ...\n",
       "9995  9995         0.396492\n",
       "9996  9996         0.386148\n",
       "9997  9997         0.401665\n",
       "9998  9998         0.414575\n",
       "9999  9999         0.381214\n",
       "\n",
       "[10000 rows x 2 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ss"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 376.72351,
   "end_time": "2022-02-02T09:49:17.361579",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2022-02-02T09:43:00.638069",
   "version": "2.3.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
